El objetivo de este trabajo práctico es el desarrollo y evaluación de una herramienta de OCR (Optical Character Recognition) para el reconocimiento de digitos manuscritos en imágenes. \par
\indent Para poder desarrollar dicha herramienta disponemos de una base de datos llamada train, que contiene  42,000 imágenes y cada una de estas es una fila de 784 pixeles. \par
\indent Lo importante de esta base de datos es que para cada imagen sabemos precisamente que digito manuscrito representa, en otras palabra a que clase pertenece. Esta informacion nos sirve para poder estimar la clase de una imagen desconocida. La idea que utilizamos para poder hacer esto es la de los k vecinos mas cercanos, que abreviamos como kNN. \par
\indent El algoritmo kNN tiene una complejidad temporal muy costosa, por lo cual antes de utilizar este algoritmo primero transformamos nuestros datos con un método llamado PCA(Principal Component Analisis) para despues poder reducir el tamaño de los datos con los que estamos trabajando.\par

\section{k-NN}
Las imagen con las que trabajamos se pueden ver como un vector X = ($x_{1}$,..,$x_{i}$,..,$x_{784}$) de 784 elementos, donde el elemento $x_i$ es el i-ésimo pixel de la imagen.\par
\indent Si tenemos una imagen X cuya clase es desconocida y 2 imágenes $Y_1$ y $Y_2$ cuyas clases son conocidas, entonces lo primero que haria el algoritmo kNN es calcula las normas $d_1$=$||X-Y_1||$ y $d_2$=$||X-Y_2||$. \par
\indent Sabemos que la norma de un vector es su longitud, entonces es intuitivo pensar que si $d_1$, la distancia entre X e $Y_1$, es menor a $d_2$, la distancia entre X e $Y_2$, entonces es mas probable que la imagen X pertenezca a la misma clase que $Y_1$. Este es el criterio utilizado por el algoritmo kNN para predecir la clase de X.  \par
\indent En este ejemplo tenemos solo dos imégenes etiquetadas, $Y_1$ y $Y_2$, con lo cual solo es lógico ver cual de las dos imágenes esta mas cerca a X para predecir a que clase pertenece, pero en este trabajo tenemos 42,000 imagenes etiquetadas en la base de datos train por lo cual tiene sentido que el algoritmo kNN se fije a que clase pertenecen las k imágenes mas parecidas a X de las 42,000 que tenemos disponibles, y que despues haga una predicción basada en que clase es la mas repetida dentro de estas k imágenes.\par

\section{PCA}
Si vemos a los pixeles de las imágenes que estamos analizando como variables aleatorias, entonces una imagen seria un vector de 784 variables aleatorias y nuestra base de datos train seria un conjunto de muestras de estos vectores. \par
\indent Suele pasar que en un vector de muchas variables aleatorias haya varias que no ofrezcan mucha información. En otras palabras, de todas las muestras que tenemos puede haber muchas variables cuyo valor no varie mucho, es decir que la varianza, en términos probabilisticos, sea muy cercana o igual a cero. \par
\indent Estas variables no nos aportan mucha informacion y no tiene mucho sentido que desperdiciemos tiempo con estos datos. El metodo PCA es una forma de transformar(proyectar) nuestros datos para que las variables aleatoria con mas varianza tomen prioridad sobre las que tienen menor varianza.\par
\indent El primer paso de PCA es el calculo de la matriz de covarianza. Si llamamos X a la matriz que tiene como filas a todas las imagenes de la base de datos train, lo primero que hacemos es vecotrizar a las columnas de X y luego multiplicamos a $\hat{X}$ por su transpuesta. Entonces la matriz de covarianza M $\in$ $R^{nxn}$  seria M=$\hat{X}^{t}*\hat{X}$. \par
\indent Una propiedad importante de esta matriz es que es simetrica, lo cual sabemos que implica que es ortogonalmente diagonalizable. Esto es importante ya que nos asegura que vamos a tener una base de autovectores la cual va a ser la matriz de la transformacion lineal que le vamos a aplicar a $\hat{X}$ antes de usar kNN. \par
\indent Para conseguir la base de autovectores de la matriz de covarianza vamos a utilizar un metodo iterativo conocido como metodo de la potencia. \par
\indent Para que este metodo funcione necesitamos que la matriz M cumpla dos cosas, que tenga n autovectores l.i y ademas que existan un autovalor $\lambda_1$ tal que $|\lambda_1|$ $>$ $|\lambda_2|$ $\geq$ $|\lambda_3|$ $\geq$ ... $\geq$ $|\lambda_n|$. \par
\indent Como M es simetrica sabemos que tiene una base l.i de autovectores pero no sabemos nada de los autovalores, entonces vamos a tenemos que asumir que va a exsistir un autovalor cuyo valor absoluto es mayor estricto a todos los otros para poder usar el metodo de la potencia.\par
\break
Intro teorica, version willy:
Para el algormitmo knn, consideramos una reduccion de las dimensiones de los datos tantos por costo temporal como la maldision de la dimensionalidad (overfitting). Para esto trabajaremos con la matriz de covarianza y hallaremos los autovectores y autovalores(de mayor modulo) los cuales al proyectar un dato vectorizado hacen que la variable se maximize y tengamos una menor perdida de informacion al reducir de dimension el dato, obteniendo asi un algoritmo de clasificacion menos costoso.
